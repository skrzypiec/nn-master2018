\documentclass[]{article}
\usepackage{mathtools}
\usepackage[pdftex]{graphicx}	
\usepackage{amsmath,amsfonts,amsthm}	
\usepackage{tikz}
\usetikzlibrary{chains, positioning}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\usepackage{sidecap}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\usetikzlibrary{calc,arrows}

%opening
\title{Activation functions}
\author{Rafa\l \vspace{1cm} Skrzypiec}
\date{}
\begin{document}
\maketitle

\section{Activation functions}

Activation function is a function that acts on every unit in neural network and gives the output value for neuron's input or set of inputs. 

The perceptron, which is a progenitor and inspiration of artificial neural network was constructed as a simplified model of a biological neuron. In neuroscience, each neuron is connected with other neurons via connections that conduct electrical impulses. If the sum of the input signals surpasses a certain threshold neuron transmits electrical signal. Translating it to perceptron idea, only if the sum of input values is greater than zero, signal is propagated through neuron. Thus perceptron uses activation function that is shown in left upper corner of Fig.~\ref{fig:activations} - Heaviside step function. The single-layered perceptron is the simplest example of feed-forward neural network. 

However it is not commonly used activation function, it does not include highly desirable property. One of the most important properties of activation functions is nonlinearity, that is the attribute that gives neural network nonlinear capabilities [Lecun]. 


\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{activations14_12}
	\caption{Set of several activation functions.}
	\label{fig:activations}
\end{figure}



\subsection{Sigmoid}

A typical choice of activation function is sigmoid. It is nonlinear function that is monotonically increasing. Logistic sigmoid function is defined by equation

\begin{equation}
\sigma(x) = \frac{1}{1+e^{-x}}.
\end{equation}

It is continuously differentiable what makes possible to use gradient-based optimization methods. Derivative with respect to $x$ is given by simple relation

\begin{equation}
\frac{d}{dx} \sigma(x)= \sigma(x)\left(1 - \sigma(x)\right).
\end{equation}

[zwiazek z tanh, wspomniec kiedy uzywa sie czego albo wykasowac obrazek]

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{sigmoid14_8}
	\caption{Example of sigmoidal function - sigmoid function, $\sigma(x) = \frac{1}{1+e^{-x}}$}
\end{figure} 

\newpage

\subsection{Probabilistic interpretation of sigmoid}


The application of sigmoid as an activation function arises naturally as the form of the posterior probability distribution in Bayesian treatment of two-class classification problem.
%
Let us consider a single-layer network and the concept of a discriminant function $y(x)$, such that the vector $x$ is assigned to class $C_1$ if $y(x) > 0$ and to class $C_2$ if $y(x) < 0$.
\\

In the simplest, linear form, discriminant function can be written as:
%
\begin{equation}
y(x) = w^\mathsf{T} x + b_0.
\end{equation}
%
We should refer to d-dimensional vector $w$ as the weight vector and the parameter $b_0$ as the bias.
There are several ways to generalize such functions, here we consider a function $g(\cdot)$ called activation function that acts on a aforementioned linear sum, and gives a discrimination function of the form
%
\begin{equation}
y = g\left(w^\mathsf{T} x + b_0 \right)
\end{equation}
%
\def\layersep{2.5cm}

	\begin{SCfigure}
		\centering

	\begin{tikzpicture}
	[   cnode/.style={draw=black,draw=black,fill=#1,minimum width=8mm,circle},
	]
	\tikzset{normal arrow/.style={draw,-latex}}
	\node[cnode=white,label=90:Output] (s) at (5,-3.2) {$ g\left(w^\mathsf{T} x + b_0 \right)$};
	\node at (0,-3.9) {$\vdots$};
	
	\node[cnode=white,label=180:Bias] (x-5) at (0,-6) {1};
	
	\foreach \x in {1,...,4}
	{
		\pgfmathparse{\x<4 ? \x : "n"}	   
		\ifnum \x = 4
		\node[cnode=white,label=180:Input \#n] (x-\x) at (0,{-1*\x-div(\x,4)}) {$x_{n}$};
		
		\else
		
		\node[cnode=white,label=180:Input \#\pgfmathresult] (x-\x) at (0,{-1*\x-div(\x,4)}) {$x_{\x}$};
		\fi
		\path[normal arrow] (x-\x) -- node[above,sloped,pos=0.4] {$w_{\pgfmathresult}$} (s);
	}
	\path[normal arrow] (x-5) -- node[above,sloped,pos=0.4] {$b_0$} (s);	
	\end{tikzpicture}
	\caption{ Representation of discriminant function $y(x)$ as a neural network diagram, having $n$ inputs, bias term and one output.  }
	\end{SCfigure}

%
Assumption that probability distribution functions of data given the class $C_k$ are given by Gaussian distributions with equal covariance matrices $ \Sigma_1 = \Sigma_2 = \Sigma$ gives:
%
\begin{equation}
p(x|C_k) = \frac{1}{\left(2 \pi\right)^{\frac{d}{2}} \left| \Sigma \right|^{\frac{1}{2}}} \exp \left[ -\frac{1}{2} \left(x - \mu_k\right)^\mathsf{T} \Sigma^{-1} \left(x - \mu_k \right) \right].
\end{equation}
%
The posterior probability of class $C_1$ can be written using Bayes' theorem:
%
\begin{eqnarray}
	p(C_1 | x) &=& \frac{p(x|C_1) p(C_1)}{ p(x|C_1) p(C_1) + p(x|C_2)p(C_2)} \nonumber\\
			   &=& \frac{1}{1 + \frac{p(x|C_2)p(C_2)}{p(x|C_1)p(C_1)}} \nonumber\\ 
			   &=& \frac{1}{1 + \exp(-a)},
\end{eqnarray}
%
where
%
\begin{eqnarray}
 a &=& \ln \frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)} \nonumber \\
 &=& \left( \mu_1 - \mu_2 \right)^{\mathsf{T}} \Sigma^{-1} x  - \frac{1}{2} \mu_1^\mathsf{T}\mu_1 + \frac{1}{2} \mu_2^\mathsf{T} \Sigma^{-1} \mu_2 + \ln \frac{p(C_1)}{p(C_2)},
\end{eqnarray}
%
hence
%
\begin{subequations}
	\begin{align}
		w &= \Sigma^{-1} \left(\mu_1 - \mu_2\right)\\
		b_0 &= - \frac{1}{2} \mu_1^\mathsf{T}\mu_1 + \frac{1}{2} \mu_2^\mathsf{T} \Sigma^{-1} \mu_2 + \ln \frac{p(C_1)}{p(C_2)}
	\end{align}
\end{subequations}

Thus the network output is given by a sigmoid activation function acting on a weighted linear combination of inputs. This reasoning can be generalized to multi-layered network. Then outputs of each hidden neuron with logistic sigmoid activation function can be interpreted as probabilities of the presence of corresponding attribute conditioned by the inputs. 




\end{document}
